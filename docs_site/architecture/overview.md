# ğŸ—ï¸ DataDistiller AI - System Architecture

## Overview

DataDistiller AI is a **Retrieval-Augmented Generation (RAG)** system that combines document retrieval with large language models to provide intelligent question-answering capabilities.

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface                           â”‚
â”‚                    (Streamlit Web App)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG Pipeline Core                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Document   â”‚  â”‚  Processing  â”‚  â”‚   Knowledge Graph   â”‚   â”‚
â”‚  â”‚  Ingestion  â”‚â†’ â”‚  & Chunking  â”‚â†’ â”‚   Construction      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Embedding   â”‚  â”‚ Vector Store â”‚  â”‚   LLM Integration   â”‚   â”‚
â”‚  â”‚ Generation  â”‚â†’ â”‚   (FAISS)    â”‚â†’ â”‚     (Ollama)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Document Ingestion (`src/ingestion/`)

**Purpose**: Load and parse documents from various formats

**Components**:
- PDF Loader (PyPDF)
- DOCX Loader (python-docx)
- Text Loader
- HTML Loader (BeautifulSoup4)
- Markdown Loader

**Flow**:
```
Documents â†’ Format Detection â†’ Content Extraction â†’ Text Output
```

### 2. Text Processing (`src/processing/`)

**Purpose**: Split documents into semantic chunks for better retrieval

**Components**:
- Semantic Chunker
- Overlap Strategy
- Metadata Preservation

**Flow**:
```
Raw Text â†’ Semantic Splitting â†’ Chunks with Context â†’ Metadata Tagging
```

**Parameters**:
- Chunk Size: ~500 tokens
- Overlap: ~50 tokens
- Strategy: Sentence-aware splitting

### 3. Embedding Generation

**Purpose**: Convert text chunks into vector representations

**Model**: `sentence-transformers/all-MiniLM-L6-v2`
- Dimensions: 384
- Speed: ~1000 sentences/sec on CPU
- Quality: Optimized for semantic similarity

**Flow**:
```
Text Chunks â†’ Sentence Transformer â†’ 384-dim Vectors â†’ Normalized
```

### 4. Vector Store (`src/retrieval/`)

**Purpose**: Efficient similarity search over document embeddings

**Technology**: FAISS (Facebook AI Similarity Search)
- Index Type: Flat (exact search)
- Distance Metric: Cosine similarity
- Persistence: Local disk storage

**Flow**:
```
Query â†’ Embedding â†’ FAISS Search â†’ Top-K Results â†’ Re-ranking
```

### 5. Knowledge Graph (`src/knowledge_graph.py`)

**Purpose**: Extract and visualize concept relationships

**Components**:
- Entity Extraction (spaCy NER)
- Relationship Detection
- Graph Construction (NetworkX)
- Visualization (PyVis)

**Modes**:
1. **Network Graph**: Interactive concept network
2. **Statistics**: Entity frequencies and metrics
3. **Semantic Flow**: Concept progression through documents
4. **AI Progression**: LLM-enhanced concept relationships

**Flow**:
```
Documents â†’ NER (spaCy) â†’ Entity Pairs â†’ Graph â†’ Visualization
```

### 6. LLM Integration (`src/llm/`)

**Purpose**: Generate natural language answers from retrieved context

**Supported Backends**:
- **Ollama** (Primary): Local, privacy-first
  - Models: qwen2.5:3b, llama2, mistral, etc.
  - API: REST (localhost:11434)
- **Claude** (Optional): Anthropic cloud API
- **Gemini** (Optional): Google cloud API

**Flow**:
```
Query + Context â†’ Prompt Template â†’ LLM â†’ Answer + Citations
```

### 7. RAG Pipeline (`src/workflows/`)

**Purpose**: Orchestrate the end-to-end RAG process

**Steps**:
1. **Index Phase**:
   ```
   Documents â†’ Ingestion â†’ Chunking â†’ Embedding â†’ Vector Store
   ```

2. **Query Phase**:
   ```
   User Query â†’ Embedding â†’ Retrieval â†’ Context Assembly â†’ LLM â†’ Answer
   ```

**Prompt Template**:
```
Context: [Retrieved document chunks]
Question: [User query]
Instructions: Answer based on context, cite sources
```

## Data Flow

### Document Indexing

```
1. User uploads documents (PDF, DOCX, TXT, etc.)
   â†“
2. Document Loader extracts text
   â†“
3. Text Chunker splits into semantic units
   â†“
4. Sentence Transformer creates embeddings
   â†“
5. FAISS indexes vectors for fast search
   â†“
6. spaCy extracts entities for Knowledge Graph
   â†“
7. Metadata stored for source tracking
```

### Query Processing

```
1. User submits natural language question
   â†“
2. Question embedded into vector
   â†“
3. FAISS finds top-K similar chunks
   â†“
4. Context assembled from retrieved chunks
   â†“
5. Prompt constructed with context + question
   â†“
6. LLM generates answer
   â†“
7. Sources cited in response
   â†“
8. Answer displayed to user
```

## Technology Stack

### NLP & ML
- **sentence-transformers**: Semantic embeddings
- **FAISS**: Vector similarity search
- **spaCy**: Named Entity Recognition (NER)
- **NetworkX**: Graph analysis
- **LangChain**: LLM orchestration

### LLM
- **Ollama**: Local model serving
- **Anthropic SDK**: Claude integration
- **Google GenAI**: Gemini integration

### UI & API
- **Streamlit**: Interactive web interface
- **FastAPI**: REST API (V2)

### Document Processing
- **PyPDF**: PDF parsing
- **python-docx**: DOCX parsing
- **BeautifulSoup4**: HTML parsing

## Performance Characteristics

### Indexing (V1)
- PDF: ~2-3 pages/sec
- DOCX: ~5-10 pages/sec
- TXT: ~20-30 pages/sec

### Query (V1)
- Embedding: ~50ms
- Vector search: ~10-50ms (depends on corpus size)
- LLM generation: ~2-5 sec (depends on model)
- **Total**: ~2-6 seconds per query

### Scalability
- **V1**: Up to ~1000 documents, single user
- **V2**: Unlimited documents, multiple concurrent users

## Security & Privacy

### V1 (Local)
- âœ… 100% local processing
- âœ… No data sent to cloud
- âœ… Ollama models run on-device
- âœ… FAISS index stored locally

### V2 (Production)
- âš ï¸ Data stored in PostgreSQL/MinIO
- âš ï¸ Cloud LLMs send data to API providers
- âœ… Can use Ollama for privacy
- âœ… Secure by design (auth pending)

## Directory Structure

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ ingestion/              # Document loaders
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ docx_loader.py
â”‚   â””â”€â”€ text_loader.py
â”œâ”€â”€ processing/             # Text chunking
â”‚   â””â”€â”€ chunker.py
â”œâ”€â”€ retrieval/              # Vector store
â”‚   â””â”€â”€ vector_store.py
â”œâ”€â”€ llm/                    # LLM integrations
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”œâ”€â”€ claude_client.py
â”‚   â””â”€â”€ gemini_client.py
â”œâ”€â”€ workflows/              # RAG orchestration
â”‚   â””â”€â”€ rag_pipeline.py
â”œâ”€â”€ knowledge_graph.py      # Graph features
â”œâ”€â”€ workflows_ollama.py     # Ollama workflow
â”œâ”€â”€ workflows_claude.py     # Claude workflow
â””â”€â”€ workflows_gemini.py     # Gemini workflow
```

## Future Enhancements

### Planned
- [ ] Hybrid search (BM25 + dense)
- [ ] Multi-document conversations
- [ ] Better chunking strategies
- [ ] Query expansion
- [ ] Answer quality metrics
- [ ] Caching layer
- [ ] Batch processing

### Under Consideration
- [ ] Multi-modal support (images, tables)
- [ ] Real-time document updates
- [ ] Distributed vector stores
- [ ] Fine-tuned embeddings
- [ ] Custom LLM integration

---

For implementation details, see the [source code](../src/) and [examples](../examples/).
