"""
FastAPI Application - DataDistiller 2.0
Asynchronous document upload and query API
"""

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Depends, Header, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import os
import asyncio
import uuid
import logging
from pathlib import Path
import tempfile
from datetime import datetime

from api.models import (
    UploadResponse, JobStatusResponse, JobListResponse,
    QueryRequest, QueryResponse, HealthResponse, ErrorResponse,
    JobStatus
)
from api.v2.upload import upload_document_to_kafka
from api.v2.query import query_documents
from storage.clients import PostgresClient, MinIOClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="DataDistiller 2.0 API",
    description="Asynchronous RAG platform with Kafka-based processing",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify actual origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize clients (lazy loaded)
_postgres_client = None
_minio_client = None

def get_postgres_client():
    """Get or create PostgreSQL client"""
    global _postgres_client
    if _postgres_client is None:
        _postgres_client = PostgresClient()
    return _postgres_client

def get_minio_client():
    """Get or create MinIO client"""
    global _minio_client
    if _minio_client is None:
        _minio_client = MinIOClient()
    return _minio_client


def require_api_key(
    x_api_key: str = Header(default=None),
    authorization: str = Header(default=None)
):
    """Optional API key auth. Enforced only if DATADISTILLER_API_KEY is set."""
    required = os.getenv("DATADISTILLER_API_KEY")
    if not required:
        return

    if x_api_key and x_api_key == required:
        return

    if authorization and authorization.startswith("Bearer "):
        token = authorization.split("Bearer ", 1)[-1].strip()
        if token == required:
            return

    raise HTTPException(status_code=401, detail="Invalid or missing API key")


@app.get("/", response_model=dict)
async def root():
    """Root endpoint"""
    return {
        "message": "DataDistiller 2.0 API",
        "version": "2.0.0",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    services = {}
    
    # Check PostgreSQL
    try:
        db = get_postgres_client()
        conn = db.get_connection()
        conn.close()
        services["postgres"] = "connected"
    except Exception as e:
        services["postgres"] = f"error: {str(e)}"
    
    # Check MinIO
    try:
        minio = get_minio_client()
        services["minio"] = "connected"
    except Exception as e:
        services["minio"] = f"error: {str(e)}"
    
    # Check Kafka (will be added)
    services["kafka"] = "not implemented"
    
    # Overall status
    status = "healthy" if all(v == "connected" or v == "not implemented" for v in services.values()) else "degraded"
    
    return HealthResponse(
        status=status,
        version="2.0.0",
        services=services
    )


@app.post("/api/v2/documents/upload", response_model=UploadResponse, status_code=202, dependencies=[Depends(require_api_key)])
async def upload_document(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = None
):
    """
    Upload document for asynchronous processing
    
    Returns 202 Accepted immediately, processing happens in background via Kafka
    """
    logger.info(f"üì§ Upload request: {file.filename} ({file.content_type})")
    
    # Validate file type - check extension instead of MIME type for better compatibility
    allowed_extensions = ['.pdf', '.txt', '.docx', '.md', '.html']
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type. Allowed: {', '.join(allowed_extensions)}"
        )
    
    try:
        # Generate job ID
        job_id = str(uuid.uuid4())
        
        # Read file content
        file_content = await file.read()
        file_size = len(file_content)
        
        # Upload to MinIO
        minio = get_minio_client()
        s3_key = f"uploads/{job_id}/{file.filename}"
        s3_info = minio.upload_bytes(file_content, s3_key, file.content_type)
        
        # Create database record
        db = get_postgres_client()
        job_data = {
            'job_id': job_id,
            'filename': file.filename,
            'file_size': file_size,
            'content_type': file.content_type,
            's3_bucket': s3_info['bucket'],
            's3_key': s3_info['key'],
            'status': 'pending'
        }
        db.create_job(job_data)
        
        # Send to Kafka (async processing)
        upload_document_to_kafka(job_id, job_data)
        
        logger.info(f"‚úÖ Upload accepted: job_id={job_id}")
        
        return UploadResponse(
            job_id=job_id,
            status=JobStatus.PENDING,
            message="Document upload accepted. Processing will begin shortly.",
            filename=file.filename,
            file_size=file_size
        )
    
    except Exception as e:
        logger.error(f"‚ùå Upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v2/documents/status/{job_id}", response_model=JobStatusResponse, dependencies=[Depends(require_api_key)])
async def get_job_status(job_id: str):
    """
    Get processing status for a job
    """
    logger.info(f"üìä Status request: job_id={job_id}")
    
    try:
        db = get_postgres_client()
        job = db.get_job_status(job_id)
        
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return JobStatusResponse(**job)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Status query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v2/documents/list", response_model=JobListResponse, dependencies=[Depends(require_api_key)])
async def list_jobs(limit: int = 100, status: str = None):
    """
    List recent document processing jobs
    """
    logger.info(f"üìã List jobs: limit={limit}, status={status}")
    
    try:
        db = get_postgres_client()
        jobs = db.list_jobs(limit=limit, status=status)
        
        job_responses = [JobStatusResponse(**job) for job in jobs]
        
        return JobListResponse(
            jobs=job_responses,
            total=len(job_responses)
        )
    
    except Exception as e:
        logger.error(f"‚ùå List jobs failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v2/query", response_model=QueryResponse, dependencies=[Depends(require_api_key)])
async def query(request: QueryRequest):
    """
    Query indexed documents
    """
    logger.info(f"üîç Query: {request.query}")
    
    try:
        start_time = datetime.now()
        
        # Execute query
        result = query_documents(
            query=request.query,
            top_k=request.top_k,
            retrieval_method=request.retrieval_method
        )
        
        end_time = datetime.now()
        latency = int((end_time - start_time).total_seconds() * 1000)
        
        query_id = str(uuid.uuid4())
        
        return QueryResponse(
            query_id=query_id,
            question=request.query,
            answer=result['answer'],
            sources=result['sources'],
            retrieval_method=request.retrieval_method,
            latency_ms=latency
        )
    
    except Exception as e:
        logger.error(f"‚ùå Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.websocket("/api/v2/ws/jobs/{job_id}")
async def job_status_ws(websocket: WebSocket, job_id: str, api_key: str = None):
    """WebSocket stream for job status updates."""
    required = os.getenv("DATADISTILLER_API_KEY")
    if required and api_key != required:
        await websocket.accept()
        await websocket.close(code=1008)
        return

    await websocket.accept()

    try:
        while True:
            db = get_postgres_client()
            job = db.get_job_status(job_id)

            if not job:
                await websocket.send_json({"error": "Job not found"})
                await websocket.close()
                return

            await websocket.send_json(job)

            if job.get("status") in ["completed", "failed"]:
                await websocket.close()
                return

            await asyncio.sleep(1.0)
    except WebSocketDisconnect:
        return


# Metrics endpoint (for Prometheus)
@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    # TODO: Implement Prometheus metrics
    return {"message": "Metrics endpoint - to be implemented"}


def main():
    """Run the FastAPI application"""
    logger.info("=" * 70)
    logger.info("üöÄ Starting DataDistiller 2.0 API")
    logger.info("=" * 70)
    
    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )


if __name__ == "__main__":
    main()
