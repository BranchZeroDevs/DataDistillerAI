# Multi-LLM Integration Guide

Your DataDistillerAI now supports multiple LLM backends! Choose the one that best fits your needs.

## ğŸ¯ Quick Comparison

| Feature | Ollama | Claude | Gemini |
|---------|--------|--------|--------|
| **Cost** | ğŸŸ¢ FREE | ğŸ’° Paid | ğŸŸ¡ Free tier |
| **Speed** | âš¡ Fast (local) | âš¡ Fast (API) | ğŸ”µ Medium |
| **Quality** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Privacy** | ğŸ” 100% Private | â˜ï¸ Cloud | â˜ï¸ Cloud |
| **Setup** | Easy | Easy | Easy |
| **Offline** | âœ… Yes | âŒ No | âŒ No |
| **Best For** | Local/Private | Complex Analysis | Budget-Friendly |

## ğŸš€ Setup Instructions

### Option 1: Ollama (100% Free, Local)

Already working! Keep running:
```bash
ollama serve
```

Test:
```bash
python test_ollama.py
```

### Option 2: Claude (Professional Quality)

**Step 1:** Get API key
- Visit: https://console.anthropic.com/
- Create account and generate API key

**Step 2:** Install and configure
```bash
python setup_claude.py
# Paste your API key when prompted
```

**Step 3:** Test
```bash
python test_claude.py
```

### Option 3: Google Gemini (Free Tier)

**Step 1:** Get API key
- Visit: https://ai.google.dev/
- Click "Get API Key" (no payment required)

**Step 2:** Install and configure
```bash
python setup_gemini.py
# Paste your API key when prompted
```

**Step 3:** Test
```bash
python test_gemini.py
```

## ğŸ’» Code Examples

### Using Ollama (Local)
```python
from src.llm_ollama import OllamaClient

client = OllamaClient(model="qwen2.5")
response = client.generate("Explain AI")
```

### Using Claude (Professional)
```python
from src.llm_claude import ClaudeClient

client = ClaudeClient()
response = client.generate("Explain AI")
```

### Using Gemini (Free Tier)
```python
from src.llm_gemini import GeminiClient

client = GeminiClient()
response = client.generate("Explain AI")
```

### Full RAG Pipeline

#### With Ollama:
```python
from src.workflows_ollama import RAGPipeline

pipeline = RAGPipeline()
pipeline.index_documents()
answer = pipeline.query("What is machine learning?")
```

#### With Claude:
```python
from src.workflows_claude import RAGPipeline

pipeline = RAGPipeline()
pipeline.index_documents()
answer = pipeline.query("What is machine learning?")
```

#### With Gemini:
```python
from src.workflows_gemini import RAGPipeline

pipeline = RAGPipeline()
pipeline.index_documents()
answer = pipeline.query("What is machine learning?")
```

## ğŸ“ Files Per Backend

### Ollama
- `src/llm_ollama.py` - Client
- `src/workflows_ollama.py` - RAG Pipeline
- `test_ollama.py` - Tests
- `setup_ollama.py` - Setup
- `OLLAMA_GUIDE.md` - Documentation

### Claude
- `src/llm_claude.py` - Client
- `src/workflows_claude.py` - RAG Pipeline
- `test_claude.py` - Tests
- `setup_claude.py` - Setup

### Gemini
- `src/llm_gemini.py` - Client
- `src/workflows_gemini.py` - RAG Pipeline (if created)
- `test_gemini.py` - Tests
- `setup_gemini.py` - Setup

## ğŸ”„ Switching Backends

Just import from different modules:

```python
# Option 1: Local Ollama
from src.workflows_ollama import RAGPipeline

# Option 2: Cloud Claude  
from src.workflows_claude import RAGPipeline

# Option 3: Free Gemini
from src.workflows_gemini import RAGPipeline

# Use the same interface
pipeline = RAGPipeline()
pipeline.index_documents()
answer = pipeline.query("Your question")
```

## ğŸ’¡ Recommendations

### Use Ollama If:
- âœ… Want 100% free solution
- âœ… Need complete privacy
- âœ… Have local compute resources
- âœ… Want no API key management

### Use Claude If:
- âœ… Need best quality answers
- âœ… Have complex reasoning tasks
- âœ… Want professional-grade output
- âœ… Don't mind paying for quality

### Use Gemini If:
- âœ… Want free cloud solution
- âœ… Have generous free tier needs
- âœ… OK with rate limiting
- âœ… Prefer Google's models

## âš™ï¸ Configuration

Add to `.env` file:

```bash
# Ollama (local, no key needed)
# Just ensure: ollama serve is running

# Claude (paid)
ANTHROPIC_API_KEY=sk-ant-xxxxx

# Gemini (free tier)
GOOGLE_API_KEY=AIzaxxxxx

# OpenAI (paid, if using)
OPENAI_API_KEY=sk-proj-xxxxx
```

## ğŸ§ª Test All Backends

```bash
# Test each one
python test_ollama.py    # Local
python test_claude.py    # Professional
python test_gemini.py    # Free
```

## ğŸ¯ Common Tasks

### Question Answering with RAG
```python
from src.workflows_claude import RAGPipeline

pipeline = RAGPipeline()
pipeline.index_documents()
answer = pipeline.query("Your question", top_k=3)
print(answer)
```

### Document Summarization
```python
from src.workflows_claude import RAGPipeline

pipeline = RAGPipeline()
summary = pipeline.summarize()
print(summary)
```

### Custom Prompts
```python
from src.llm_claude import ClaudeClient

client = ClaudeClient()
response = client.generate(
    prompt="Your custom prompt",
    system_prompt="You are a helpful assistant",
    max_tokens=1024,
    temperature=0.7
)
```

## ğŸš€ Production Deployment

For production, consider:

1. **Ollama**: Best for on-premises, air-gapped systems
2. **Claude**: Best for professional applications that need quality
3. **Gemini**: Best for cost-conscious projects with moderate scale

All three support the same RAGPipeline interface, so switching is easy:

```python
# In production, maybe use environment variable to choose
import os

backend = os.getenv("RAG_BACKEND", "claude")

if backend == "ollama":
    from src.workflows_ollama import RAGPipeline
elif backend == "claude":
    from src.workflows_claude import RAGPipeline
else:
    from src.workflows_gemini import RAGPipeline

pipeline = RAGPipeline()
```

---

**Your DataDistillerAI supports multiple professional LLM backends!** ğŸš€
